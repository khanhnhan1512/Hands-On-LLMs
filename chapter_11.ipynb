{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b312f7d",
   "metadata": {},
   "source": [
    "# Chapter 11 - Fine-tuning Representation Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19019c27",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4967ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "tomatoes = load_dataset(\"rotten_tomatoes\")\n",
    "train_data, test_data = tomatoes[\"train\"], tomatoes[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266e50b",
   "metadata": {},
   "source": [
    "## Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc9c33",
   "metadata": {},
   "source": [
    "### HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd9a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_id = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133748ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17b178a75a44de08c7e624548ad5a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the inputs\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ab3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    load_f1 = evaluate.load(\"f1\")\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f441d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44837898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c709e1bff5a4ba0baca1f66f51d6563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3903, 'grad_norm': 15.705974578857422, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n",
      "{'train_runtime': 46.3531, 'train_samples_per_second': 184.022, 'train_steps_per_second': 11.52, 'train_loss': 0.3865355516640881, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.3865355516640881, metrics={'train_runtime': 46.3531, 'train_samples_per_second': 184.022, 'train_steps_per_second': 11.52, 'total_flos': 213940121334480.0, 'train_loss': 0.3865355516640881, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6001d4f",
   "metadata": {},
   "source": [
    "### Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071ed7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "662dd45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f752e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('classifier'):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf2c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: bert.embeddings.word_embeddings.weight ---- False\n",
      "Parameter: bert.embeddings.position_embeddings.weight ---- False\n",
      "Parameter: bert.embeddings.token_type_embeddings.weight ---- False\n",
      "Parameter: bert.embeddings.LayerNorm.weight ---- False\n",
      "Parameter: bert.embeddings.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.0.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.0.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.1.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.1.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.2.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.2.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.3.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.3.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.4.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.4.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.5.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.5.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.6.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.6.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.7.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.7.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.8.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.8.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.9.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.9.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.10.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.10.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.weight ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.bias ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.weight ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.bias ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.weight ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.bias ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.11.output.dense.weight ---- False\n",
      "Parameter: bert.encoder.layer.11.output.dense.bias ---- False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.weight ---- False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.bias ---- False\n",
      "Parameter: bert.pooler.dense.weight ---- False\n",
      "Parameter: bert.pooler.dense.bias ---- False\n",
      "Parameter: classifier.weight ---- True\n",
      "Parameter: classifier.bias ---- True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name} ---- {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe7804a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c39b2efbe04a1eaf29885ff5248997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6986, 'grad_norm': 3.4565298557281494, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n",
      "{'train_runtime': 13.3265, 'train_samples_per_second': 640.078, 'train_steps_per_second': 40.071, 'train_loss': 0.697593588954054, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.697593588954054, metrics={'train_runtime': 13.3265, 'train_samples_per_second': 640.078, 'train_steps_per_second': 40.071, 'total_flos': 213940121334480.0, 'train_loss': 0.697593588954054, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134db1f8",
   "metadata": {},
   "source": [
    "### Freeze blocks 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50465c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827ce135d7b3490481745e8976c17d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4517, 'grad_norm': 2.768611431121826, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n",
      "{'train_runtime': 18.5949, 'train_samples_per_second': 458.727, 'train_steps_per_second': 28.718, 'train_loss': 0.4488348192936472, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.4488348192936472, metrics={'train_runtime': 18.5949, 'train_samples_per_second': 458.727, 'train_steps_per_second': 28.718, 'total_flos': 213940121334480.0, 'train_loss': 0.4488348192936472, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_id = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Encoder block 10 starts at index 165 and we will freeze all layers before it\n",
    "for index, (name, param) in enumerate(model.named_parameters()):\n",
    "    if index < 165:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1cb6ce",
   "metadata": {},
   "source": [
    "### MLM (Masked Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ac23305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de79ad3cb4884b1599372ac7b523dfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\LLMs-env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3869260fc84848a7cccbc7bef54a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5feb86b5ad64be1b3910da8cea5f63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732915a3ac2b455496d986640b83af16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acd5e2fb6c64dc0bfeb8dd17a6611cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load model for Masked Language Modeling (MLM)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f692323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f97f2b5385488f937d4508f33685e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d746d1c07ed408f99a607fb5ee3c0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the inputs\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns([\"label\"])\n",
    "tokenized_test = tokenized_test.remove_columns([\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a052c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e67456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=10,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5328357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118cb549d08f4e6b840d6aa060f37523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6029, 'grad_norm': 13.746612548828125, 'learning_rate': 1.812734082397004e-05, 'epoch': 0.94}\n",
      "{'loss': 2.3769, 'grad_norm': 16.20442771911621, 'learning_rate': 1.6254681647940076e-05, 'epoch': 1.87}\n",
      "{'loss': 2.3035, 'grad_norm': 23.52048683166504, 'learning_rate': 1.4382022471910113e-05, 'epoch': 2.81}\n",
      "{'loss': 2.1871, 'grad_norm': 13.83089542388916, 'learning_rate': 1.250936329588015e-05, 'epoch': 3.75}\n",
      "{'loss': 2.148, 'grad_norm': 13.555903434753418, 'learning_rate': 1.0636704119850187e-05, 'epoch': 4.68}\n",
      "{'loss': 2.0923, 'grad_norm': 23.861364364624023, 'learning_rate': 8.764044943820226e-06, 'epoch': 5.62}\n",
      "{'loss': 2.0568, 'grad_norm': 15.416306495666504, 'learning_rate': 6.891385767790263e-06, 'epoch': 6.55}\n",
      "{'loss': 1.9921, 'grad_norm': 14.122122764587402, 'learning_rate': 5.0187265917603005e-06, 'epoch': 7.49}\n",
      "{'loss': 1.9847, 'grad_norm': 19.902162551879883, 'learning_rate': 3.146067415730337e-06, 'epoch': 8.43}\n",
      "{'loss': 1.9646, 'grad_norm': 15.873626708984375, 'learning_rate': 1.2734082397003748e-06, 'epoch': 9.36}\n",
      "{'train_runtime': 619.1176, 'train_samples_per_second': 137.777, 'train_steps_per_second': 8.625, 'train_loss': 2.158107880438758, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Save pre-trained tokenizer\n",
    "tokenizer.save_pretrained(\"mlm\")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save updated model\n",
    "model.save_pretrained(\"mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b6d450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible idea!\n",
      ">>> What a horrible dream!\n",
      ">>> What a horrible thing!\n",
      ">>> What a horrible day!\n",
      ">>> What a horrible thought!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load and create predictions\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "# Print results\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3c8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible movie!\n",
      ">>> What a horrible film!\n",
      ">>> What a horrible mess!\n",
      ">>> What a horrible story!\n",
      ">>> What a horrible comedy!\n"
     ]
    }
   ],
   "source": [
    "mask_filter = pipeline('fill-mask', model='mlm')\n",
    "preds = mask_filter(\"What a horrible [MASK]!\")\n",
    "\n",
    "# Print results\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25709020",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15aa2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ecf054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86682cada63743c8a2c9381f57655ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6915cfd0d34b66b4e60957bf30bb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d3bda24fc145328e5b7b65eb0c98df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecaf151748e458ab59380d044f650e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478c3f3326464535855e0acc7c0d358c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8113f8e1104ea9b7e4415485f0be5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The CoNLL-2003 dataset for NER\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd49b486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '848',\n",
       " 'tokens': ['Dean',\n",
       "  'Palmer',\n",
       "  'hit',\n",
       "  'his',\n",
       "  '30th',\n",
       "  'homer',\n",
       "  'for',\n",
       "  'the',\n",
       "  'Rangers',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],\n",
       " 'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],\n",
       " 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][848]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54cb4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4,\n",
    "    'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8\n",
    "}\n",
    "id2label = {index: label for label, index in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "929bc640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6265af8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Dean',\n",
       " 'Palmer',\n",
       " 'hit',\n",
       " 'his',\n",
       " '30th',\n",
       " 'home',\n",
       " '##r',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Rangers',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]\n",
    "sub_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "sub_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da972b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5ac71d5bdb4214bae82ad3c878196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a04a0b9d6b8434a9adeb3ce9873e548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e148ab9e61964c5fad7a6c2ab49d735c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def align_labels(examples):\n",
    "    token_ids = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = examples[\"ner_tags\"]\n",
    "\n",
    "    updated_labels = []\n",
    "    for index, label in enumerate(labels):\n",
    "        word_ids = token_ids.word_ids(batch_index=index) # Map tokens to their respective words\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx != previous_word_idx:\n",
    "                previous_word_idx = word_idx\n",
    "                updated_label = -100 if word_idx is None else label[word_idx]\n",
    "                label_ids.append(updated_label)\n",
    "            elif not word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                updated_label = label[word_idx]\n",
    "                if updated_label % 2 == 1:\n",
    "                    updated_label += 1\n",
    "                label_ids.append(updated_label)\n",
    "        updated_labels.append(label_ids)\n",
    "    token_ids[\"labels\"] = updated_labels\n",
    "    return token_ids\n",
    "\n",
    "tokenized = dataset.map(align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "875f2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]\n",
      "Updated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "# Difference between original and updated labels\n",
    "print(f\"Original: {example['ner_tags']}\")\n",
    "print(f\"Updated: {tokenized['train'][848]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bafa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Token-classification Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de873164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5668a344e9044318abcb3275e699712e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/878 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2158, 'grad_norm': 1.0152201652526855, 'learning_rate': 8.610478359908885e-06, 'epoch': 0.57}\n",
      "{'train_runtime': 77.3608, 'train_samples_per_second': 181.5, 'train_steps_per_second': 11.349, 'train_loss': 0.1555034576624564, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=878, training_loss=0.1555034576624564, metrics={'train_runtime': 77.3608, 'train_samples_per_second': 181.5, 'train_steps_per_second': 11.349, 'total_flos': 351240792638148.0, 'train_loss': 0.1555034576624564, 'epoch': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6ff9cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99016213,\n",
       "  'index': 1,\n",
       "  'word': 'N',\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9897619,\n",
       "  'index': 2,\n",
       "  'word': '##han',\n",
       "  'start': 1,\n",
       "  'end': 4}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Save our fine-tuned model\n",
    "trainer.save_model(\"ner_model\")\n",
    "\n",
    "# Run inference on the fine-tuned model\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"ner_model\",\n",
    ")\n",
    "token_classifier(\"Nhan is a student.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
